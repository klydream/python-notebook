(1) First, start all the necessary services using Confluent CLI
$ confluent start

(2) Start hdfs in directory hadoop-2.8.3
$ sbin/start-dfs.sh

(3) Start yarn
$ sbin/start-yarn.sh

(4)Start the Avro console producer to import a few records to Kafka
./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic test_hdfs \
--property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'
Then in the console producer, type in:
{"f1": "value1"}
{"f1": "value2"}
{"f1": "value3"}

Try others
./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic test_hdfs \
--property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}, {"name":"f2","type":"string"}]}'
Then in the console producer, type in
{"f1": "value1", "f2": "value4"}
{"f1": "value2", "f2": "value5"}
{"f1": "value3", "f2": "value6"}

./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic test_hdfs \
--property value.schema='{"namespace": "example.avro",
 "type": "record",
 "name": "User",
 "fields": [
     {"name": "name", "type": "string"},
     {"name": "favorite_number",  "type": ["int", "null"]},
     {"name": "favorite_color", "type": ["string", "null"]}
 ]
}'
Then in the console producer, type in
{"name": "Alyssa", "favorite_number": 256}
{"name": "Ben", "favorite_number": 7, "favorite_color": "red"}
{"name": "larry", "favorite_number": 7, "favorite_color": "green"}
will meet error:
{"name": "larry", "favorite_number": 7, "favorite_color": "green"}org.apache.kafka.common.errors.SerializationException: Error deserializing json {"name": "Alyssa", "favorite_number": 256} to Avro of schema {"type":"record","name":"User","namespace":"example.avro","fields":[{"name":"name","type":"string"},{"name":"favorite_number","type":["int","null"]},{"name":"favorite_color","type":["string","null"]}]}
Caused by: org.apache.avro.AvroTypeException: Expected start-union. Got VALUE_NUMBER_INT
	at org.apache.avro.io.JsonDecoder.error(JsonDecoder.java:698)
	at org.apache.avro.io.JsonDecoder.readIndex(JsonDecoder.java:441)
	at org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:290)


./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic test_hdfs \
> --property value.schema='{"namespace": "example.avro",
>  "type": "record",
>  "name": "User",
>  "fields": [
>      {"name": "name", "type": "string"},
>      {"name": "favorite_number",  "type": "int"},
>      {"name": "favorite_color", "type": "string"}
>  ]
> }'
{"name": "Alyssa", "favorite_number": 256, "favorite_color": "white"}
{"name": "Ben", "favorite_number": 7, "favorite_color": "red"}
{"name": "larry", "favorite_number": 6, "favorite_color": "green"}

(5) After installing Filebeat, you need to configure it. Open the filebeat.yml file located in your Filebeat 
installation directory
filebeat.prospectors:
- type: log
  paths:
    - /path/to/file/logstash-tutorial.log 
output.logstash:
  hosts: ["localhost:5044"]
  
(6) At the data source machine, run Filebeat with the following command:
./filebeat -e -c filebeat.yml -d "publish"

(7) Configuring Logstash for Filebeat Input
input {
    beats {
        port => "5044"
    }
}
# The filter part of this file is commented out to indicate that it is
# optional.
# filter {
#
# }

output {
  kafka {
    codec => json
    bootstrap_servers => "localhost:9092"
    topic_id => "test"
  }
}

(7) If the configuration file passes the configuration test, start Logstash with the following command:
bin/logstash -f first-pipeline.conf --config.reload.automatic
